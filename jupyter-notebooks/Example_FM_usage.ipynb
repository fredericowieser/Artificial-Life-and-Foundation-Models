{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage of some Foundation Models\n",
    "\n",
    "### WARNING (~10GB sized model being downloaded on computer)\n",
    "This notebook needs you to have the hugging face CLI set up:\n",
    "```sh\n",
    "pip install -U \"huggingface_hub[cli]\"\n",
    "```\n",
    "On Macintosh you can also do:\n",
    "```sh\n",
    "brew install huggingface-cli\n",
    "```\n",
    "\n",
    "Next you will need to go to your hugging face account and create a token so as to load the model via the python package we use in this notebook. You will find this in your user settings (https://huggingface.co/settings/tokens). After creating the \"write\" token run the CLI tool:\n",
    "```sh\n",
    "huggingface-cli login\n",
    "```\n",
    "Here paste in your token where relevant and say \"yes\"/Y to the git usage agreement.\n",
    "\n",
    "You should be ready to run this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device found.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69bdd3c39894c74ae84ff5944b1876d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer\n",
    ")\n",
    "\n",
    "# Pick your device\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"MPS device found.\")\n",
    "    device = torch.device(\"mps\") # This is for Macintosh devices\n",
    "else:\n",
    "    print(\"MPS device not found; using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Write a Python script that generates the Fibonacci "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence. The Fibonacci sequence is defined by the following recursive formula:\n",
      "\n",
      "$f(n)=\\left\\{\\begin{array}{ll} 0 & \\text { if } n=0 \\\\ 1 & \\text { if } n=1 \\\\ f(n-1)+f(n-2) & \\text { if } n>1 \\end{array}\\right. $\n",
      "\n",
      "The script should prompt the user to enter a number and then print the Fibonacci sequence up to that number.\n",
      "\n",
      "A 100-W lightbulb is placed in a cylinder equipped with a moveable piston. The lightbulb is turned on for\n",
      "--- Done ---\n"
     ]
    }
   ],
   "source": [
    "# Run the model with a prompt and stream the output on a separate thread\n",
    "prompt = \"Write a Python script that generates the Fibonacci sequence.\"\n",
    "encoded = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)\n",
    "generation_kwargs = dict(\n",
    "    **encoded,                   # expand -> input_ids=<tensor>, attention_mask=<tensor>\n",
    "    max_new_tokens=128,         # limit to avoid massive memory usage\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    streamer=streamer\n",
    ")\n",
    "def run_generation():\n",
    "    model.generate(**generation_kwargs)\n",
    "thread = Thread(target=run_generation)\n",
    "thread.start()\n",
    "print(\"Generated text:\\n\", end=\"\", flush=True)\n",
    "for new_text in streamer:\n",
    "    sys.stdout.write(new_text)\n",
    "    sys.stdout.flush()\n",
    "thread.join()\n",
    "print(\"\\n--- Done ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a class for faster testing of new ideas and quality of life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Convenience wrapper for loading a causal language model and generating text.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"google/gemma-2-2b\",\n",
    "        device=None,\n",
    "        torch_dtype=torch.float16,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=128,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param model_name: HF Hub name or local path to a model\n",
    "        :param device: 'cpu', 'cuda', or 'mps'; auto-detect if None\n",
    "        :param torch_dtype: torch.dtype for model weights (e.g., torch.float16)\n",
    "        :param do_sample: Enable sampling (vs. greedy) by default\n",
    "        :param max_new_tokens: Default max tokens to generate\n",
    "        :param temperature: Default temperature for sampling\n",
    "        :param top_k: Default top-k sampling\n",
    "        :param top_p: Default top-p (nucleus) sampling\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = (torch.device(device) if device else\n",
    "                       torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "                       else torch.device(\"cpu\"))\n",
    "        self.torch_dtype = torch_dtype\n",
    "\n",
    "        # Default generation parameters\n",
    "        self.do_sample = do_sample\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the tokenizer and model weights into memory.\"\"\"\n",
    "        print(f\"Loading '{self.model_name}' on {self.device} (dtype={self.torch_dtype})...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=self.torch_dtype\n",
    "        ).to(self.device)\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def generate_text(self, prompt, stream=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Generates text from a prompt. By default returns the entire output;\n",
    "        if stream=True, returns a generator that yields tokens as they're produced.\n",
    "\n",
    "        :param prompt: Prompt text to condition on\n",
    "        :param stream: If True, yield tokens in real-time\n",
    "        :param kwargs: Override default generation parameters\n",
    "        :return: A string if stream=False, or a generator if stream=True\n",
    "        \"\"\"\n",
    "        if not self.model or not self.tokenizer:\n",
    "            raise RuntimeError(\"Call load_model() before generate_text().\")\n",
    "\n",
    "        # Merge default params with user overrides\n",
    "        params = {\n",
    "            \"do_sample\": self.do_sample,\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"top_k\": self.top_k,\n",
    "            \"top_p\": self.top_p\n",
    "        }\n",
    "        if self.tokenizer.eos_token_id is not None:\n",
    "            params[\"eos_token_id\"] = self.tokenizer.eos_token_id\n",
    "        params.update(kwargs)\n",
    "\n",
    "        # Tokenize input\n",
    "        encoded = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Non-streaming generation\n",
    "        if not stream:\n",
    "            output_ids = self.model.generate(**encoded, **params)\n",
    "            return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Streaming generation\n",
    "        streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True)\n",
    "        params[\"streamer\"] = streamer\n",
    "\n",
    "        def run_generation():\n",
    "            self.model.generate(**encoded, **params)\n",
    "\n",
    "        thread = Thread(target=run_generation)\n",
    "        thread.start()\n",
    "\n",
    "        def token_generator():\n",
    "            for new_text in streamer:\n",
    "                yield new_text\n",
    "            thread.join()\n",
    "\n",
    "        return token_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 'google/gemma-2-2b' on mps (dtype=torch.bfloat16)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b7a7d453b141c4ba4e6ac7d7c2046f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    model_name=\"google/gemma-2-2b\",  # or another\n",
    "    torch_dtype=torch.bfloat16,      # use BF16 if available\n",
    "    temperature=0.8,\n",
    "    max_new_tokens=64\n",
    ")\n",
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short story about a robot learning to dance. Make sure you describe the robotâ€™s appearance, its actions, and how it feels.\n",
      "\n",
      "What is the difference between a <em>monomer</em> and a <em>polymer</em>? Give an example of each.\n",
      "\n",
      "Consider the following equilibrium process at $700^{\\circ} \\mathrm{C}$.\n",
      "\n",
      "$2 \\mathrm{H}_2(g)+\\mathrm{S}_2(g) \\rightleftharpoons 2 \\mathrm{H}_2 \\mathrm{~S}(g) $\n",
      "\n",
      "Analysis shows that there are $2.50$ moles of $\\mathrm{H}_2, 1.35 \\times$ $10^{-5}$ mole of $\\mathrm{S}_2$, and $8.70$ moles of $\\mathrm{H}_2 \\mathrm{S}$ present in a $12.0-\\mathrm{L}$ flask. Calculate the equilibrium constant $K_{\\mathrm{c}}$ for the reaction.\n",
      "\n",
      "A \n"
     ]
    }
   ],
   "source": [
    "output = model.generate_text(\n",
    "    \"Write a short story about a robot learning to dance.\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.5\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-streamed output:\n",
      " Explain quantum computing in simple terms.\n",
      "\n",
      "Why is the chemical energy in a battery a form of potential energy rather than kinetic energy?\n",
      "\n",
      "A circular coil $18.0 \\mathrm{~cm}$ in diameter and containing twelve loops lies flat on the ground. The Earth's magnetic field at this location has magnitude $5.50 \\times \n"
     ]
    }
   ],
   "source": [
    "# Basic generation (non-streaming)\n",
    "text = model.generate_text(\"Explain quantum computing in simple terms.\")\n",
    "print(\"Non-streamed output:\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a short poem about the moon.\n",
      "\n",
      "In this case, the poem is about the moon and its effect on the world, as well as the poem's effect on the world.\n",
      "\n",
      "The poem is about the moon and its effect on the world.\n",
      "\n",
      "The poem is about the moon and its effect on the world.\n",
      "\n",
      "The poem is about the\n",
      "--- Done streaming ---\n"
     ]
    }
   ],
   "source": [
    "# Streaming generation\n",
    "gen = model.generate_text(\"Write a short poem about the moon.\", stream=True)\n",
    "for chunk in gen:\n",
    "    print(chunk, end=\"\")\n",
    "print(\"\\n--- Done streaming ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
