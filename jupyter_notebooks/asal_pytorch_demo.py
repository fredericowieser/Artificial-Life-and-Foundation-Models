# -*- coding: utf-8 -*-
"""asal_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mI_VgI9TEfVOXLvUHT6NOIx7QsAxobu0

# ASAL IN PYTORCH
"""

!pip install evotorch

import torch
import imageio
import numpy as np
import torch.nn.functional as F
import matplotlib.pyplot as plt
from transformers import (
    AutoProcessor,
    CLIPModel,
)
from einops import rearrange, reduce, repeat
from tqdm.auto import tqdm
from PIL import Image
from torchvision import transforms
from io import BytesIO
from evotorch import Problem, SolutionBatch
from evotorch.algorithms import CMAES
from evotorch.logging import StdOutLogger

import os
import json
import pickle
import urllib.request
import requests

from typing import Any, Optional, Union, Tuple
from functools import partial
from dataclasses import dataclass
from collections import namedtuple

def inv_sigmoid(x):
    return torch.log(x) - torch.log1p(-x)

def save_json(save_dir, name, item):
    if save_dir is not None:
        os.makedirs(f"{save_dir}/", exist_ok=True)
        with open(f"{save_dir}/{name}.json", "w") as f:
            json.dump(item, f)

def load_json(load_dir, name):
    if load_dir is not None:
        with open(f"{load_dir}/{name}.json", "r") as f:
            return json.load(f)
    else:
        return None

def save_pkl(save_dir, name, item):
    if save_dir is not None:
        os.makedirs(f"{save_dir}/", exist_ok=True)
        with open(f"{save_dir}/{name}.pkl", "wb") as f:
            pickle.dump(item, f)

def load_pkl(load_dir, name):
    if load_dir is not None:
        with open(f"{load_dir}/{name}.pkl", "rb") as f:
            return pickle.load(f)
    else:
        return None

def load_pkl_from_url(url: str):
    with urllib.request.urlopen(url) as response:
        data = response.read()
    return pickle.loads(data)

class CLIP:
    def __init__(self, clip_model="clip-vit-base-patch32"):
        self.processor = AutoProcessor.from_pretrained(f"openai/{clip_model}")
        self.clip_model = CLIPModel.from_pretrained(f"openai/{clip_model}")
        self.clip_model.eval()

        self.img_mean = torch.tensor(self.processor.image_processor.image_mean).view(3, 1, 1)
        self.img_std = torch.tensor(self.processor.image_processor.image_std).view(3, 1, 1)

        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ])

    def embed_img(self, img):
        """
        img: numpy array (H, W, C) with values in [0, 1]
        returns: tensor of shape (D)
        """
        if isinstance(img, Image.Image):
            img = self.transform(img)
        else:
            img = torch.tensor(img).permute(2, 0, 1)  # Convert HWC to CHW
            img = F.interpolate(img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)

        img = (img - self.img_mean) / self.img_std
        img = img.unsqueeze(0)  # Add batch dimension

        with torch.no_grad():
            z_img = self.clip_model.get_image_features(pixel_values=img)
            z_img = F.normalize(z_img, p=2, dim=-1)

        return z_img.squeeze(0)

    def embed_txt(self, prompts):
        """
        prompts: list of strings
        returns: tensor of shape (B, D)
        """
        inputs = self.processor(text=prompts, return_tensors="pt", padding=True)

        with torch.no_grad():
            z_text = self.clip_model.get_text_features(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
            z_text = F.normalize(z_text, p=2, dim=-1)

        return z_text

fm = CLIP()

# Load image from URL
img_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/800px-Cat03.jpg"
response = requests.get(img_url, stream=True, headers={"User-Agent": "Mozilla/5.0"})
if response.status_code == 200:
    img = Image.open(BytesIO(response.content)).convert("RGB")
else:
    raise ValueError("Failed to fetch image from URL")

# Embed image and text
img_embedding = fm.embed_img(img)
text_embedding = fm.embed_txt(["A cat"])

# Compute similarity
similarity = torch.dot(img_embedding, text_embedding[0])
print("Dot product similarity:", similarity.item())

# Compute similarity
similarity = torch.dot(text_embedding[0], text_embedding[0])
print("Dot product similarity:", similarity.item())

# Compute similarity
similarity = torch.dot(img_embedding, img_embedding)
print("Dot product similarity:", similarity.item())

patterns = {}

patterns["VT049W"] = {"name": "Aquarium (self-replicating)", "R": 12, "T": 2,
		"kernels": [
			{"b": [1], "m": 0.272, "s": 0.0595, "h": 0.138, "r": 0.91, "c0": 0, "c1": 0},
			{"b": [1], "m": 0.349, "s": 0.1585, "h": 0.48, "r": 0.62, "c0": 0, "c1": 0},
			{"b": [1, 1/4], "m": 0.2, "s": 0.0332, "h": 0.284, "r": 0.5, "c0": 0, "c1": 0},
			{"b": [0, 1], "m": 0.114, "s": 0.0528, "h": 0.256, "r": 0.97, "c0": 1, "c1": 1},
			{"b": [1], "m": 0.447, "s": 0.0777, "h": 0.5, "r": 0.72, "c0": 1, "c1": 1},
			{"b": [5/6, 1], "m": 0.247, "s": 0.0342, "h": 0.622, "r": 0.8, "c0": 1, "c1": 1},
			{"b": [1], "m": 0.21, "s": 0.0617, "h": 0.35, "r": 0.96, "c0": 2, "c1": 2},
			{"b": [1], "m": 0.462, "s": 0.1192, "h": 0.218, "r": 0.56, "c0": 2, "c1": 2},
			{"b": [1], "m": 0.446, "s": 0.1793, "h": 0.556, "r": 0.78, "c0": 2, "c1": 2},
			{"b": [11/12, 1], "m": 0.327, "s": 0.1408, "h": 0.344, "r": 0.79, "c0": 0, "c1": 1},
			{"b": [3/4, 1], "m": 0.476, "s": 0.0995, "h": 0.456, "r": 0.5, "c0": 0, "c1": 2},
			{"b": [11/12, 1], "m": 0.379, "s": 0.0697, "h": 0.67, "r": 0.72, "c0": 1, "c1": 0},
			{"b": [1], "m": 0.262, "s": 0.0877, "h": 0.42, "r": 0.68, "c0": 1, "c1": 2},
			{"b": [1/6, 1, 0], "m": 0.412, "s": 0.1101, "h": 0.43, "r": 0.82, "c0": 2, "c1": 0},
			{"b": [1], "m": 0.201, "s": 0.0786, "h": 0.278, "r": 0.82, "c0": 2, "c1": 1}],
		"cells": [
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.49, 1.0, 0, 0.03, 0.49, 0.49, 0.28, 0.16, 0.03, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.6, 0.47, 0.31, 0.58, 0.51, 0.35, 0.28, 0.22, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.15, 0.32, 0.17, 0.61, 0.97, 0.29, 0.67, 0.59, 0.88, 1.0, 0.92, 0.8, 0.61, 0.42, 0.19, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.25, 0.64, 0.26, 0.92, 0.04, 0.24, 0.97, 1.0, 1.0, 1.0, 1.0, 0.97, 0.71, 0.33, 0.12, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.38, 0.84, 0.99, 0.78, 0.67, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 0.62, 0.37, 0, 0], [0, 0, 0, 0, 0.04, 0.11, 0, 0.69, 0.75, 0.75, 0.91, 1.0, 1.0, 0.89, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81, 0.42, 0.07, 0], [0, 0, 0, 0, 0.44, 0.63, 0.04, 0, 0, 0, 0.11, 0.14, 0, 0.05, 0.64, 1.0, 1.0, 1.0, 1.0, 1.0, 0.92, 0.56, 0.23, 0], [0, 0, 0, 0, 0.11, 0.36, 0.35, 0.2, 0, 0, 0, 0, 0, 0, 0.63, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.49, 0.26, 0], [0, 0, 0, 0, 0, 0.4, 0.37, 0.18, 0, 0, 0, 0, 0, 0.04, 0.41, 0.52, 0.67, 0.82, 1.0, 1.0, 0.91, 0.4, 0.23, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04, 0, 0.05, 0.45, 0.89, 1.0, 0.66, 0.35, 0.09, 0], [0, 0, 0.22, 0, 0, 0, 0.05, 0.36, 0.6, 0.13, 0.02, 0.04, 0.24, 0.34, 0.1, 0, 0.04, 0.62, 1.0, 1.0, 0.44, 0.25, 0, 0], [0, 0, 0, 0.43, 0.53, 0.58, 0.78, 0.9, 0.96, 1.0, 1.0, 1.0, 1.0, 0.71, 0.46, 0.51, 0.81, 1.0, 1.0, 0.93, 0.19, 0.06, 0, 0], [0, 0, 0, 0, 0.23, 0.26, 0.37, 0.51, 0.71, 0.89, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42, 0.06, 0, 0, 0], [0, 0, 0, 0, 0.03, 0, 0, 0.11, 0.35, 0.62, 0.81, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 0.64, 0.15, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.06, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.05, 0.09, 0.05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.02, 0.28, 0.42, 0.44, 0.34, 0.18, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.34, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91, 0.52, 0.14, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01, 0.17, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 0.35, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.22, 0.92, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.59, 0.09], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71, 0.16], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01, 0.67, 0.83, 0.85, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.68, 0.17], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.21, 0.04, 0.12, 0.58, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 0.57, 0.13], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.07, 0, 0, 0, 0.2, 0.64, 0.96, 1.0, 1.0, 1.0, 0.9, 0.24, 0.01], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13, 0.29, 0, 0, 0, 0.25, 0.9, 1.0, 1.0, 1.0, 1.0, 0.45, 0.05, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13, 0.31, 0.07, 0, 0.46, 0.96, 1.0, 1.0, 1.0, 1.0, 0.51, 0.12, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.26, 0.82, 1.0, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3, 0.05, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.28, 0.74, 1.0, 0.95, 0.87, 1.0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.07, 0.69, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96, 0.25, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.4, 0.72, 0.9, 0.83, 0.7, 0.56, 0.43, 0.14, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04, 0.25, 0.37, 0.44, 0.37, 0.24, 0.11, 0.04, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.19, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.4, 0.15, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.14, 0.48, 0.83, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.62, 0.78, 0.94, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.64, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.02, 0.65, 0.98, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.78, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.15, 0.48, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.79, 0.05, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.33, 0.56, 0.8, 1.0, 1.0, 1.0, 0.37, 0.6, 0.94, 1.0, 1.0, 1.0, 1.0, 0.68, 0.05, 0, 0, 0], [0, 0, 0, 0, 0.35, 0.51, 0.76, 0.89, 1.0, 1.0, 0.72, 0.15, 0, 0.29, 0.57, 0.69, 0.86, 1.0, 0.92, 0.49, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.38, 0.86, 1.0, 1.0, 0.96, 0.31, 0, 0, 0, 0, 0.02, 0.2, 0.52, 0.37, 0.11, 0, 0, 0, 0], [0, 0, 0.01, 0, 0, 0.07, 0.75, 1.0, 1.0, 1.0, 0.48, 0.03, 0, 0, 0, 0, 0, 0.18, 0.07, 0, 0, 0, 0, 0], [0, 0.11, 0.09, 0.22, 0.15, 0.32, 0.71, 0.94, 1.0, 1.0, 0.97, 0.54, 0.12, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0.06, 0.33, 0.47, 0.51, 0.58, 0.77, 0.95, 1.0, 1.0, 1.0, 1.0, 0.62, 0.12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0.04, 0.4, 0.69, 0.88, 0.95, 1.0, 1.0, 1.0, 1.0, 1.0, 0.93, 0.68, 0.22, 0.02, 0, 0, 0.01, 0, 0, 0, 0, 0, 0, 0], [0, 0.39, 0.69, 0.91, 1.0, 1.0, 1.0, 1.0, 1.0, 0.85, 0.52, 0.35, 0.24, 0.17, 0.07, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.29, 0.82, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.67, 0.29, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.2, 0.51, 0.77, 0.96, 0.93, 0.71, 0.4, 0.16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.08, 0.07, 0.03, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
			]
}

patterns["5N7KKM"] = {"name": "Aquarium (swarm)", "R": 12, "T": 2,
		"kernels": [
			{"b": [1], "m": 0.22, "s": 0.0628, "h": 0.174, "r": 0.87, "c0": 0, "c1": 0},
			{"b": [1], "m": 0.351, "s": 0.1539, "h": 0.46, "r": 0.52, "c0": 0, "c1": 0},
			{"b": [1, 1/4], "m": 0.177, "s": 0.0333, "h": 0.31, "r": 0.58, "c0": 0, "c1": 0},
			{"b": [0, 1], "m": 0.126, "s": 0.0525, "h": 0.242, "r": 0.89, "c0": 1, "c1": 1},
			{"b": [1], "m": 0.437, "s": 0.0797, "h": 0.508, "r": 0.78, "c0": 1, "c1": 1},
			{"b": [3/4, 1], "m": 0.234, "s": 0.0369, "h": 0.566, "r": 0.79, "c0": 1, "c1": 1},
			{"b": [1], "m": 0.179, "s": 0.0653, "h": 0.406, "r": 1.0, "c0": 2, "c1": 2},
			{"b": [1], "m": 0.489, "s": 0.1213, "h": 0.27, "r": 0.64, "c0": 2, "c1": 2},
			{"b": [1], "m": 0.419, "s": 0.1775, "h": 0.588, "r": 0.96, "c0": 2, "c1": 2},
			{"b": [11/12, 1], "m": 0.341, "s": 0.1388, "h": 0.294, "r": 0.66, "c0": 0, "c1": 1},
			{"b": [3/4, 1], "m": 0.469, "s": 0.1054, "h": 0.388, "r": 0.69, "c0": 0, "c1": 2},
			{"b": [1, 11/12], "m": 0.369, "s": 0.0721, "h": 0.62, "r": 0.61, "c0": 1, "c1": 0},
			{"b": [1], "m": 0.219, "s": 0.0898, "h": 0.348, "r": 0.81, "c0": 1, "c1": 2},
			{"b": [1/6, 1], "m": 0.385, "s": 0.1102, "h": 0.436, "r": 0.81, "c0": 2, "c1": 0},
			{"b": [1], "m": 0.208, "s": 0.0749, "h": 0.39, "r": 0.71, "c0": 2, "c1": 1}],
		"cells": [
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.12, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.49, 0, 0, 0, 0, 0, 0, 0.23, 0.47, 0.31, 0.93, 0.75, 0, 0, 0, 0], [0, 0, 0, 0.23, 0, 0, 0.65, 0.68, 0.12, 0, 0, 0, 0.02, 0.40, 0.82, 0.86, 0, 0.19, 0, 0], [0, 0, 0.01, 0.01, 0.77, 1.00, 0.98, 1.00, 0.97, 0.85, 0.70, 0.55, 0.12, 0.15, 0.01, 0, 0, 0.95, 0, 0], [0, 0, 0, 0.66, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.74, 0.76, 0.27, 0, 0, 0.18, 0.59, 0.31, 0], [0, 0.04, 0.08, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.88, 0.68, 0.07, 0, 0, 0, 0, 0, 0], [0, 0, 0.29, 1.00, 1.00, 1.00, 1.00, 0.90, 1.00, 0.92, 0.58, 0.84, 0.89, 0.39, 0, 0, 0.04, 1.00, 0, 0], [0, 0.06, 0.27, 1.00, 1.00, 1.00, 0.82, 0.39, 0, 0, 0, 0.12, 0.87, 0.70, 0.58, 0.04, 0.40, 1.00, 0.35, 0], [0, 0.21, 0.38, 1.00, 1.00, 0.66, 0, 0, 0, 0, 0, 0, 1.00, 0.79, 0.74, 0.16, 0.31, 0.42, 0, 0], [0, 0.26, 0.50, 1.00, 1.00, 0.46, 0, 0, 0, 0, 0, 0.40, 1.00, 1.00, 0.71, 0.16, 0, 0.22, 0, 0], [0, 0.14, 0.48, 1.00, 1.00, 0.77, 0, 0, 0, 0, 0, 1.00, 1.00, 1.00, 0.79, 0, 0, 0, 0, 0], [0, 0, 0.16, 1.00, 1.00, 1.00, 0.19, 0, 0, 0.09, 0.20, 0.57, 1.00, 1.00, 0.74, 0, 0, 0, 0, 0], [0, 0, 0, 0.59, 1.00, 1.00, 0.85, 0.75, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.47, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.95, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.96, 0.44, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.25, 0.79, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.96, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.04, 0.06, 0.26, 0.61, 1.00, 1.00, 1.00, 1.00, 1.00, 0, 0, 0.32, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.15, 0, 0.02, 0.23, 0.24, 0.05, 0, 0, 0.25, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.02, 0.04, 0, 0, 0.08, 0, 0, 0, 0, 0, 0, 0]],
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.03, 0.43, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.14, 0.47, 0, 0, 0.27, 0.92, 0.87, 0.70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.86, 1.00, 0.66, 1.00, 1.00, 1.00, 1.00, 0.33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1.00, 1.00, 1.00, 1.00, 1.00, 0.13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1.00, 1.00, 1.00, 1.00, 1.00, 0, 0.22, 0.30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.76, 1.00, 1.00, 1.00, 1.00, 1.00, 0.83, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.71, 1.00, 1.00, 1.00, 1.00, 0.77, 0.81, 0.75, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.69, 1.00, 1.00, 1.00, 0.88, 0.24, 0.35, 0.62, 0.35, 0.09, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.29, 1.00, 1.00, 1.00, 0.98, 0.38, 0.13, 0.65, 0.88, 0.32, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0.09, 1.00, 1.00, 1.00, 0.93, 0.77, 0.88, 0.24, 0.03, 0.69, 1.00, 0.78, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.44, 1.00, 1.00, 1.00, 1.00, 1.00, 0.76, 0.83, 1.00, 0.92, 0.17, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.36, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.12, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.06, 0.39, 0.79, 1.00, 1.00, 1.00, 0.48, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.16, 0.59, 1.00, 1.00, 1.00, 0.13, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04, 0.16, 0.02, 0, 0, 0, 0, 0, 0, 0, 0]],
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0.51, 0.46, 0.26, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.38, 1.00, 1.00, 0.96, 0.85, 0.57, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.06, 0.63, 1.00, 1.00, 1.00, 1.00, 0.96, 0.70, 0.08, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.01, 0.36, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.82, 0.49, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.05, 0.50, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.99, 0.65, 0.04, 0, 0, 0, 0], [0, 0, 0.01, 0, 0.08, 0.52, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.93, 0, 0, 0, 0, 0], [0, 0, 0.03, 0, 0.09, 0.49, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.58, 0.04, 0, 0, 0, 0, 0], [0, 0, 0.02, 0, 0.08, 0.50, 1.00, 1.00, 1.00, 1.00, 1.00, 0.81, 0, 0, 0, 0, 0.08, 0.01, 0, 0], [0, 0, 0, 0, 0.04, 0.47, 1.00, 1.00, 1.00, 1.00, 0.40, 0, 0, 0, 0, 0.77, 0.85, 0.35, 0, 0], [0, 0, 0, 0, 0.03, 0.41, 1.00, 1.00, 1.00, 1.00, 0.20, 0, 0, 0.01, 1.00, 1.00, 1.00, 0.55, 0, 0], [0, 0, 0, 0, 0, 0.30, 0.98, 1.00, 1.00, 1.00, 0.68, 0.09, 0.26, 1.00, 1.00, 1.00, 1.00, 0.70, 0.19, 0], [0, 0, 0, 0, 0, 0.13, 0.55, 0.95, 1.00, 1.00, 1.00, 0.89, 1.00, 1.00, 1.00, 1.00, 1.00, 0.85, 0.67, 0.24], [0, 0, 0, 0, 0, 0.02, 0.31, 0.63, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.88, 0.45], [0, 0, 0, 0, 0, 0, 0.12, 0.44, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.91, 0.36], [0, 0, 0, 0, 0, 0, 0, 0.16, 0.48, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.86, 0.06], [0, 0, 0, 0, 0, 0, 0, 0, 0.13, 0.32, 0.70, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.28, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.06, 0.18, 0.32, 0.57, 0.90, 1.00, 1.00, 1.00, 0.08, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04, 0.12, 0.25, 0.39, 0.31, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
			]
}

patterns["5N7KKM_2"] = {"name": "Aquarium (swarm, twin)", "R": 12, "T": 2,
		"cells": [
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01, 0.15, 0.17, 0.02, 0, 0, 0.06, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.19, 0.46, 0.39, 0.49, 1.00, 1.00, 1.00, 0.61, 0.28, 0.21, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0.30, 0, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.59, 0.11, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.79, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.78, 0.88, 1.00, 1.00, 1.00, 0.56, 0.07, 0.03, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0.18, 0.88, 0.97, 0.99, 1.00, 1.00, 1.00, 0.23, 0, 0, 0.13, 0.91, 1.00, 1.00, 1.00, 0.46, 0.10, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.75, 0.48, 0.42, 0.58, 1.00, 1.00, 1.00, 0, 0, 0, 0, 0.10, 0.95, 1.00, 1.00, 1.00, 0.84, 0.21, 0.07, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0.01, 0, 0, 0, 0.70, 1.00, 0, 0, 0, 0, 0, 0, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.32, 0.13, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.16, 0.46, 0.12, 0.11, 0.22, 0.07, 0, 0, 0.72, 0.95, 1.00, 1.00, 1.00, 1.00, 1.00, 0.29, 0.14, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.17, 1.00, 0.90, 0.41, 0.09, 0, 0, 0, 0.41, 1.00, 1.00, 1.00, 1.00, 0.06, 0.15, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0.02, 0, 0.33, 0.70, 1.00, 1.00, 0.74, 0.33, 0.03, 0, 0, 0.58, 1.00, 1.00, 1.00, 0.27, 0, 0.12, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.71, 1.00, 0.95, 0.68, 0.61, 0.44, 0.19, 0.14, 0.92, 1.00, 1.00, 1.00, 0.05, 0, 0, 0.01, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.38, 0.94, 0.87, 0.04, 0.06, 0.49, 0.47, 0.71, 1.00, 1.00, 1.00, 1.00, 0, 0, 0.04, 0.17, 0.03, 0.02, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0.28, 0.71, 0, 0, 0, 0.09, 0.31, 0.35, 0.18, 0.20, 0.58, 0.84, 0, 0, 0.34, 1.00, 0.68, 0.08, 0.07, 0.16, 0.09, 0.04, 0, 0, 0, 0], [0, 0, 0, 0, 0.03, 0.01, 0.34, 0.03, 0.16, 0, 0, 0, 0, 0, 0, 0, 0.51, 1.00, 1.00, 0.30, 0, 0, 0, 0.10, 0.17, 0.14, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.49, 0.94, 0.99, 0.21, 0, 0, 0.19, 0.80, 0.91, 0.73, 0.27, 0.10, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.38, 0.64, 0, 0, 0, 1.00, 1.00, 1.00, 1.00, 1.00, 0.94, 0.30, 0.05, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.42, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.88, 0.22, 0.04], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.62, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.47, 0.01], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.68, 1.00, 1.00, 0.50, 0.58, 1.00, 1.00, 1.00, 1.00, 1.00, 0.88, 0.09], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.66, 1.00, 0.03, 0, 0.05, 0.94, 1.00, 0.81, 0.99, 1.00, 1.00, 0.21], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.52, 0.54, 0.07, 0, 0.22, 0.73, 0, 0, 0.55, 1.00, 1.00, 0.30], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.51, 0.69, 0.29, 0, 0, 0, 0, 0, 0.39, 1.00, 1.00, 0.19], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14, 0.88, 0.44, 0, 0, 0, 0, 0, 0.61, 1.00, 1.00, 0.06], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.29, 1.00, 0.62, 0.02, 0, 0, 0, 0.34, 1.00, 1.00, 0.97, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.05, 0, 0.35, 0.91, 1.00, 1.00, 0.29, 0.09, 0.08, 1.00, 1.00, 1.00, 1.00, 0.66, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01, 0.53, 0.87, 0.82, 1.00, 0.59, 0.16, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.06, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.61, 0.63, 0.20, 0.18, 0, 0.22, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.65, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.55, 0.96, 0.97, 1.00, 1.00, 1.00, 1.00, 0.15, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.80, 1.00, 1.00, 1.00, 0.13, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.26, 0.85, 1.00, 1.00, 0, 0, 0.05, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.53, 0.61, 0, 0, 0, 0.05, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.28, 0, 0, 0, 0, 0]],
			[[0, 0, 0, 0, 0, 0, 0.37, 0.46, 0.45, 0.37, 0.34, 0.13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.11, 0.83, 0.83, 0.97, 1.00, 1.00, 1.00, 0.39, 0.02, 0, 0, 0.10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.13, 0.86, 0.64, 0.14, 0.54, 0.48, 0.45, 0.37, 0.27, 0.79, 0.46, 0.12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.30, 0.69, 0, 0, 0.07, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.78, 0.21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.27, 0.51, 0, 0, 0.01, 0.90, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.84, 0.19, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.05, 1.00, 1.00, 0.68, 0.68, 1.00, 1.00, 1.00, 0.99, 1.00, 1.00, 1.00, 0.53, 0.01, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.18, 1.00, 1.00, 1.00, 0.60, 0.15, 0.98, 1.00, 1.00, 0.90, 1.00, 1.00, 1.00, 0.43, 0.04, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.83, 0.88, 0.96, 1.00, 1.00, 0.16, 0, 0.94, 1.00, 1.00, 1.00, 1.00, 1.00, 0.57, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.60, 0, 0, 0, 0, 0.11, 0.52, 1.00, 1.00, 1.00, 1.00, 1.00, 0.89, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.21, 0.84, 1.00, 1.00, 1.00, 1.00, 0.89, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15, 0.61, 0.61, 0.69, 0.82, 0.87, 0.22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.07, 0.68, 0.20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04, 0.12, 0.57, 0.68, 0.26, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.30, 0.91, 1.00, 1.00, 1.00, 0.82, 0.11, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.84, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.33, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.58, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.53, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.71, 1.00, 1.00, 1.00, 0.94, 1.00, 1.00, 1.00, 0.68, 0.03], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.79, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.51, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.08, 0.63, 0.98, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.12, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.26, 0.70, 0.74, 0.78, 0.93, 1.00, 1.00, 0.78, 0.11, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.30, 0.23, 0.34, 0.61, 0.84, 1.00, 0.60, 0.09, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.53, 0.58, 0.30, 0.78, 1.00, 0.74, 0.33, 0.05, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.07, 1.00, 1.00, 1.00, 0.57, 0.62, 0.83, 0.43, 0.07, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.61, 1.00, 1.00, 0, 0, 0.62, 1.00, 0.98, 0.24, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.78, 0.81, 0.23, 0, 0.32, 1.00, 1.00, 1.00, 0.28, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.40, 0.09, 0.10, 0.20, 1.00, 1.00, 0.95, 1.00, 0.99, 0.09, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.47, 0, 0, 0.59, 1.00, 0.50, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.58, 0.50, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0.01, 0.09, 0.17, 0.07, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.18, 0.40, 0.70, 1.00, 0.86, 0.39, 0.14, 0.07, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0.08, 0.62, 1.00, 1.00, 1.00, 1.00, 1.00, 0.86, 0.77, 0.63, 0.40, 0.13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.93, 0.59, 0.34, 0.25, 0.07, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0.87, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.98, 0.89, 0.52, 0.17, 0.03, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0.27, 0.87, 1.00, 1.00, 1.00, 0.90, 0.95, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.56, 0.30, 0.14, 0.01, 0, 0, 0, 0, 0, 0, 0, 0], [0.29, 0.85, 1.00, 1.00, 0.95, 0.89, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.87, 0.72, 0.50, 0.19, 0, 0, 0, 0, 0, 0, 0, 0], [0.31, 0.81, 0.99, 1.00, 0.92, 0.96, 1.00, 1.00, 1.00, 1.00, 0.84, 0.75, 0.87, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.65, 0.19, 0, 0, 0, 0, 0, 0, 0], [0.19, 0.49, 0.79, 0.99, 1.00, 1.00, 1.00, 1.00, 1.00, 0.55, 0.15, 0.21, 0.65, 0.75, 0.62, 0.78, 1.00, 1.00, 1.00, 1.00, 1.00, 0.65, 0.11, 0, 0, 0, 0, 0, 0], [0, 0.17, 0.69, 0.88, 0.96, 1.00, 1.00, 1.00, 0.27, 0, 0, 0, 0, 0, 0, 0.87, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.40, 0.01, 0, 0, 0, 0, 0], [0, 0, 0.32, 0.53, 0.75, 0.95, 1.00, 0.23, 0, 0, 0, 0, 0, 0, 0.80, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.76, 0.16, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0.28, 0.46, 0.27, 0.15, 0, 0, 0, 0, 0, 0.95, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.98, 0.35, 0.04, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.10, 0.79, 1.00, 1.00, 1.00, 1.00, 0.96, 1.00, 1.00, 1.00, 1.00, 1.00, 0.94, 0.45, 0.07, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.72, 0.86, 0.98, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.90, 0.51, 0.09, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.11, 0.44, 0.73, 0.95, 1.00, 1.00, 1.00, 1.00, 1.00, 0.84, 0.93, 1.00, 1.00, 0.75, 0.15, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.40, 0.82, 1.00, 1.00, 1.00, 0.75, 0.25, 0.51, 0.98, 1.00, 1.00, 1.00, 0.25, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.08, 0.68, 0.97, 1.00, 0, 0, 0, 0.84, 1.00, 1.00, 1.00, 1.00, 0.29, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.57, 0.31, 0, 0, 0, 0.50, 0.91, 1.00, 1.00, 1.00, 0.97, 0.34, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.41, 0.67, 0.99, 1.00, 1.00, 0.98, 0.43, 0.02, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.32, 0.84, 1.00, 1.00, 1.00, 0.97, 0.47, 0.04, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24, 0.72, 0.98, 1.00, 1.00, 1.00, 1.00, 0.50, 0.03, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14, 0.69, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.43, 0.01, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.17, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.38, 0.02, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.78, 1.00, 1.00, 1.00, 1.00, 1.00, 0.97, 1.00, 1.00, 1.00, 0.51, 0.05, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.62, 0.95, 1.00, 0.98, 0.95, 0.83, 0.89, 1.00, 1.00, 1.00, 0.36, 0, 0, 0.01, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.50, 0.88, 0.95, 0.87, 0.86, 0.90, 1.00, 1.00, 1.00, 0.85, 0.08, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.49, 0.87, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.28, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.31, 0.73, 0.99, 1.00, 1.00, 1.00, 1.00, 1.00, 0.51, 0.03, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.03, 0.52, 0.84, 0.97, 1.00, 1.00, 1.00, 0.56, 0.07, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13, 0.40, 0.66, 0.86, 0.95, 0.26, 0, 0, 0, 0, 0, 0, 0, 0]],
			]
}

patterns["5N7KKM_g"] = {"name": "Aquarium (swarm, gyrating)", "R": 12, "T": 2,
		"cells": [
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.04, 0.01, 0, 0.06, 0.13, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.02, 0.19, 0.43, 0.53, 0.69, 0.80, 0.51, 0, 0, 0, 0.16, 0.11, 0, 0, 0, 0, 0], [0, 0, 0, 0.11, 0.47, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0, 0, 0, 0.07, 0, 0, 0], [0, 0, 0, 0.57, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.97, 0.93, 0, 0, 0, 0, 0], [0, 0.13, 0.49, 1.00, 1.00, 0.90, 0.59, 0, 0, 0, 0, 0.12, 1.00, 1.00, 1.00, 1.00, 0.94, 0.69, 0, 0.54, 0, 0], [0.06, 0.21, 0.65, 1.00, 1.00, 1.00, 0.76, 0, 0, 0, 0, 0, 0.67, 1.00, 1.00, 1.00, 0.91, 0.38, 0, 0.04, 0.27, 0], [0.06, 0.24, 0.75, 1.00, 1.00, 1.00, 0.98, 0.05, 0, 0, 0, 0, 0.40, 0.74, 1.00, 1.00, 0.89, 0.27, 0, 0, 0, 0], [0.08, 0.27, 0.78, 1.00, 1.00, 1.00, 1.00, 0.05, 0, 0, 0, 0, 0.34, 0.51, 0.72, 0.93, 0.81, 0.35, 0, 0, 0, 0], [0.11, 0.24, 0.71, 1.00, 1.00, 1.00, 0.98, 0, 0, 0, 0, 0.14, 0.53, 0.57, 0.52, 0.37, 0.68, 0.48, 0.07, 0, 0, 0], [0, 0.12, 0.26, 0.98, 1.00, 1.00, 1.00, 1.00, 1.00, 0.78, 0.75, 1.00, 1.00, 1.00, 0.84, 0.18, 0.33, 0.45, 0.15, 0, 0, 0], [0, 0.02, 0.11, 0.50, 0.86, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.96, 0.79, 0.20, 0.16, 0.33, 0.17, 0, 0, 0], [0, 0, 0.04, 0.14, 0.28, 0.46, 0.43, 0.88, 1.00, 1.00, 1.00, 1.00, 0.78, 0.71, 0.63, 0.05, 0, 0.23, 0.32, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.34, 0.86, 0.32, 0.02, 0.02, 0.03, 0.02, 0.05, 0.07, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09, 0.70, 0.27, 0, 0, 0.25, 0.20, 0.10, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.37, 1.00, 0.35, 0.22, 0.49, 0, 0, 0.79, 0.67, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1.00, 0.14, 0.12, 0.61, 0, 0, 0.83, 0.34, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.55, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23, 0.32, 0.09, 0.03, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.51, 0.97, 1.00, 0.91, 0.58, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0.44, 0.88, 1.00, 0.72, 0.38, 0.48, 0.64, 0.59, 0.62, 0.99, 0.77, 0.50, 0, 0, 0, 0, 0], [0, 0, 0, 0.47, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.46, 0.37, 0.71, 0.68, 0.21, 0, 0, 0, 0, 0], [0, 0.15, 0.61, 1.00, 1.00, 1.00, 1.00, 0.97, 0.85, 0.67, 0.39, 0.57, 0.99, 0.91, 0.99, 0.85, 0.17, 0, 0, 0, 0, 0], [0.03, 0.35, 0.93, 1.00, 1.00, 1.00, 1.00, 0.97, 0.84, 0.62, 0.13, 0.24, 0.99, 1.00, 0.94, 0.95, 0.52, 0, 0, 0, 0, 0], [0.11, 0.56, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.90, 0.66, 0.05, 0.04, 0.73, 0.71, 0.55, 0.88, 0.70, 0, 0, 0, 0, 0], [0.19, 0.83, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.83, 0.43, 0, 0, 0.40, 0.71, 0.60, 0.52, 0.28, 0, 0, 0, 0, 0], [0.18, 0.81, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.80, 0.30, 0, 0, 0, 0.30, 0.69, 0.86, 0.39, 0, 0, 0, 0, 0], [0.08, 0.47, 0.99, 1.00, 1.00, 1.00, 1.00, 0.55, 0.13, 0, 0, 0, 0, 0, 0, 0.40, 0, 0, 0, 0, 0, 0], [0, 0.26, 0.63, 1.00, 1.00, 1.00, 1.00, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0.06, 0.28, 0.60, 0.65, 0.80, 0.79, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
			[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01, 0.01, 0.01, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01, 0.02, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.07, 0.10, 0.11, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.09, 0.17, 0.34, 0.43, 0.48, 0.22, 0.06, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.11, 0.25, 0.38, 0.56, 0.68, 1.00, 1.00, 1.00, 1.00, 0.95, 0.33, 0.18, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0.19, 0.42, 0.69, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.74, 0.49, 0.02, 0, 0], [0, 0, 0, 0, 0, 0.01, 0.11, 0.45, 0.64, 0.88, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.89, 0.26, 0, 0], [0, 0, 0, 0, 0.08, 0.73, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.91, 0.36, 0], [0, 0, 0, 0.06, 0.29, 1.00, 1.00, 1.00, 1.00, 0.94, 0.63, 0.61, 0.91, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.64, 0], [0, 0.01, 0.07, 0.40, 0.73, 1.00, 1.00, 1.00, 1.00, 1.00, 0.23, 0, 0.05, 0.71, 1.00, 1.00, 1.00, 1.00, 1.00, 0.99, 0.90, 0.39], [0, 0.03, 0.12, 0.51, 0.90, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.51, 0.16, 0.28, 1.00, 0.96, 0.99, 1.00, 1.00, 0.94, 0.83, 0.59], [0, 0.03, 0.13, 0.56, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.30, 0.28, 0.40, 0.80, 0.88, 0.98, 0.95, 0.83, 0.71, 0.52], [0, 0, 0.05, 0.42, 0.83, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.24, 0, 0.55, 0.64, 0.71, 0.62, 0.48, 0.38, 0.19], [0, 0, 0, 0.23, 0.54, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.98, 0.68, 0, 0.43, 0.58, 0.55, 0.38, 0.25, 0.16, 0], [0, 0, 0, 0, 0, 0.19, 0.52, 1.00, 0.99, 0.86, 0.66, 0.60, 0.55, 0.51, 0, 0, 0.05, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0.35, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
			]
}

patterns["5N7KKM_2"]["kernels"] = patterns["5N7KKM"]["kernels"]
patterns["5N7KKM_g"]["kernels"] = patterns["5N7KKM"]["kernels"]

Carry = namedtuple('Carry', [ 'world', 'param', 'asset', 'temp' ])
Accum = namedtuple('Accum', [ 'phenotype', 'stats' ])
Param = namedtuple('Params', [ 'm', 's', 'h' ])
Asset = namedtuple('Asset', [ 'fK', 'X', 'reshape_c_k', 'reshape_k_c', 'R', 'T' ])
Temp = namedtuple('Temp', [ 'last_center', 'last_shift', 'total_shift', 'last_angle' ])
Stats = namedtuple('Stats', [ 'mass', 'center_x', 'center_y', 'linear_velocity', 'angle', 'angular_velocity', 'is_empty', 'is_full', 'is_spread' ])
Others = namedtuple('Others', [ 'D', 'K', 'cells', 'init_cells' ])

bell = lambda x, mean, stdev: torch.exp(-((x - mean) / stdev) ** 2 / 2)
growth = lambda x, mean, stdev: 2 * bell(x, mean, stdev) - 1

@dataclass
class ConfigLenia:
	# Init pattern
	pattern_id: str = "VT049W"

	# World
	world_size: int = 128
	world_scale: int = 1

	# Simulation
	n_step: int = 200

	# Genotype
	n_params_size: int = 3
	n_cells_size: int = 32

class LeniaImpl:
    def __init__(self, config):
        """
        PyTorch implementation of Lenia Initialization
        """
        self._config = config
        self.pattern = patterns[self._config.pattern_id]

        # Genotype
        self.n_kernel = len(self.pattern["kernels"])  # Number of kernels (k)
        self.n_channel = len(self.pattern["cells"])  # Number of channels (c)
        self.n_params = self._config.n_params_size * self.n_kernel  # Total genotype parameters
        self.n_cells = self._config.n_cells_size ** 2 * self.n_channel  # Total number of embryo cells
        self.n_gene = self.n_params + self.n_cells  # Total genotype size

    def create_world_from_cells(self, cells):
        """
        Create a world tensor from the given cell configuration.
        """
        mid = self._config.world_size // 2

        # Scale the cells
        scaled_cells = cells.repeat((self._config.world_scale, self._config.world_scale, 1))  # Repeat along spatial dims
        cy, cx = scaled_cells.shape[:2]

        # Create an empty world tensor
        A = torch.zeros((self._config.world_size, self._config.world_size, self.n_channel), dtype=torch.float32)

        # Place scaled cells in the center of the world
        A[mid - cx // 2: mid + cx - cx // 2, mid - cy // 2: mid + cy - cy // 2, :] = scaled_cells

        return A

    def load_pattern(self, pattern):
        """
        Load a pattern and preprocess its parameters for simulation.
        """
        # Unpack pattern data
        cells = torch.tensor(pattern['cells'], dtype=torch.float32).permute(1, 2, 0)  # (y, x, c)
        kernels = pattern['kernels']
        R = pattern['R'] * self._config.world_scale
        T = pattern['T']

        # Extract kernel parameters
        m = torch.tensor([k['m'] for k in kernels], dtype=torch.float32)  # (k,)
        s = torch.tensor([k['s'] for k in kernels], dtype=torch.float32)  # (k,)
        h = torch.tensor([k['h'] for k in kernels], dtype=torch.float32)  # (k,)
        init_params = torch.vstack([m, s, h])  # (p, k)

        # Generate reshaping arrays
        reshape_c_k = torch.zeros((self.n_channel, self.n_kernel))  # (c, k)
        reshape_k_c = torch.zeros((self.n_kernel, self.n_channel))  # (k, c)
        for i, k in enumerate(kernels):
            reshape_c_k[k['c0'], i] = 1.0
            reshape_k_c[i, k['c1']] = 1.0

        # Compute kernel-related matrices
        mid = self._config.world_size // 2
        x_range = torch.arange(-mid, mid, dtype=torch.float32) / R
        X = torch.meshgrid(x_range, x_range, indexing='ij')  # (y, x) coordinates
        X = torch.stack(X, dim=0)
        D = torch.sqrt(X[0]**2 + X[1]**2)  # Distance matrix
        Ds = [D * len(k['b']) / k['r'] for k in kernels]  # Scaled distances per kernel
        Ks = [(D < len(k['b'])) * torch.tensor(k['b'])[torch.clamp(D.long(), max=len(k['b'])-1)] * bell(D % 1, 0.5, 0.15) for D, k in zip(Ds, kernels)]
        K = torch.stack(Ks, dim=-1)  # (y, x, k)
        nK = K / K.sum(dim=(0, 1), keepdim=True)  # Normalize kernels
        fK = torch.fft.fft2(torch.fft.fftshift(nK, dim=(0, 1)), dim=(0, 1))  # FFT of kernels

        # Pad pattern cells into initial cells
        cy, cx = cells.shape[:2]
        py, px = self._config.n_cells_size - cy, self._config.n_cells_size - cx
        init_cells = F.pad(cells, (0, 0, px//2, px-px//2, py//2, py-py//2))  # (e, e, c)

        # Create world from initial cells
        A = self.create_world_from_cells(init_cells)

        # Pack initial data
        init_carry = Carry(
            world=A,
            param=Param(m, s, h),
            asset=Asset(fK, X, reshape_c_k, reshape_k_c, R, T),
            temp=Temp(torch.zeros(2), torch.zeros(2, dtype=torch.int), torch.zeros(2, dtype=torch.int), 0.0),
        )
        init_genotype = torch.cat([init_params.flatten(), init_cells.flatten()])
        other_asset = Others(D, K, cells, init_cells)
        return init_carry, init_genotype, other_asset

    def express_genotype(self, carry, genotype):
        """
        Express the genotype by reshaping parameters and generating the world state.
        """
        params = genotype[:self.n_params].reshape((self._config.n_params_size, self.n_kernel))
        cells = genotype[self.n_params:].reshape((self._config.n_cells_size, self._config.n_cells_size, self.n_channel))

        m, s, h = params
        A = self.create_world_from_cells(cells)

        carry = carry._replace(world=A)
        carry = carry._replace(param=Param(m, s, h))
        return carry

    def step(self, carry, unused, phenotype_size, center_phenotype, record_phenotype):
        """
        Perform a single Lenia step using PyTorch
        """
        # Unpack data from last step
        A = carry.world
        m, s, h = carry.param
        fK, X, reshape_c_k, reshape_k_c, R, T = carry.asset
        last_center, last_shift, total_shift, last_angle = carry.temp
        m = m.unsqueeze(0).unsqueeze(0)  # (1, 1, k)
        s = s.unsqueeze(0).unsqueeze(0)  # (1, 1, k)
        h = h.unsqueeze(0).unsqueeze(0)  # (1, 1, k)

        # Center world for accurate calculation of center and velocity
        shifts = [-x for x in last_shift.tolist()]
        A = torch.roll(A, shifts=shifts, dims=(-3, -2))

        # Lenia step
        fA = torch.fft.fft2(A, dim=(-3, -2))
        reshape_c_k = reshape_c_k.to(torch.complex64)
        fA_k = torch.einsum('yxc,ck->yxk', fA, reshape_c_k)
        U_k = torch.real(torch.fft.ifft2(fK * fA_k, dim=(-3, -2)))
        G_k = growth(U_k, m, s) * h
        G = torch.einsum('yxk,kc->yxc', G_k, reshape_k_c)
        next_A = torch.clamp(A + (1 / T) * G, 0, 1)

        # Calculate center
        m00 = A.sum()
        sum2d = next_A.sum(dim=-1)         # [y, x]
        sum2d = sum2d.unsqueeze(0)         # [1, y, x]
        AX = sum2d * X                     # [2, y, x], if X is [2, y, x]
        center = AX.sum(dim=(-2, -1)) / m00
        shift = (center * R).to(torch.int)
        total_shift = total_shift + shift

        # Get phenotype
        if record_phenotype:
            if center_phenotype:
                phenotype = next_A
            else:
                phenotype = torch.roll(next_A, shifts=(total_shift - shift).tolist(), dims=(0, 1))
            mid = self._config.world_size // 2
            half_size = phenotype_size // 2
            phenotype = phenotype[mid-half_size:mid+half_size, mid-half_size:mid+half_size]
        else:
            phenotype = None

        # Calculate mass and velocity
        mass = m00 / (R * R)
        actual_center = center + total_shift / R
        center_diff = center - last_center + last_shift / R
        linear_velocity = torch.linalg.norm(center_diff) * T

        # Calculate angular velocity
        angle = torch.atan2(center_diff[1], center_diff[0]) / torch.pi
        angle_diff = (angle - last_angle + 3) % 2 - 1
        angle_diff = torch.where(linear_velocity > 0.01, angle_diff, torch.tensor(0.0))
        angular_velocity = angle_diff * T

        # Check if world is empty or full
        is_empty = (next_A < 0.1).all(dim=(-3, -2)).any()
        borders = (next_A[..., 0, :, :].sum() + next_A[..., -1, :, :].sum() +
                   next_A[..., :, 0, :].sum() + next_A[..., :, -1, :].sum())
        is_full = borders > 0.1
        is_spread = A[mid-half_size:mid+half_size, mid-half_size:mid+half_size].sum() / m00 < 0.9

        # Pack data for next step
        carry = carry._replace(world=next_A)
        carry = carry._replace(temp=Temp(center, shift, total_shift, angle))
        stats = Stats(mass, actual_center[1], -actual_center[0], linear_velocity, angle, angular_velocity, is_empty, is_full, is_spread)
        accum = Accum(phenotype, stats)
        return carry, accum

class Lenia:
    """
    Higher-level Lenia class in PyTorch, mirroring the JAX-based Lenia substrate.
    """

    def __init__(
        self,
        grid_size: int = 128,
        center_phenotype: bool = True,
        phenotype_size: int = 48,
        start_pattern: str = "5N7KKM",
        clip1: float = float("inf"),
        clip2: float = float("inf")
    ):
        """
        :param grid_size:       Size of the Lenia simulation grid.
        :param center_phenotype: Whether to center the phenotype each step.
        :param phenotype_size:   The size of the cropped phenotype to record.
        :param start_pattern:     Which initial pattern to load from patterns.
        :param clip1:            Clipping range for the dynamic genotype parameters.
        :param clip2:            Clipping range for the initial cell genotype parameters.
        """
        self.grid_size = grid_size
        self.center_phenotype = center_phenotype
        self.phenotype_size = phenotype_size
        self.config_lenia = ConfigLenia(pattern_id=start_pattern, world_size=grid_size)
        self.lenia = LeniaImpl(self.config_lenia)  # Your PyTorch-based LeniaImpl

        self.clip1, self.clip2 = clip1, clip2

        # Load pattern and initialize
        init_carry, init_genotype, other_asset = self.lenia.load_pattern(self.lenia.pattern)
        self.init_carry = init_carry
        self.init_genotype = init_genotype

        # Convert the initial genotype into logit space (inv_sigmoid)
        # so it can be easily perturbed and then sigmoided back.
        self.base_params = inv_sigmoid(self.init_genotype)

    def default_params(self, rng: Optional[torch.Generator] = None) -> torch.Tensor:
        """
        Creates a default random parameter tensor around zero, with the same shape
        as self.base_params. This matches the JAX code's `random.normal(...) * 0.1`.
        """
        shape = self.base_params.shape
        # If a torch.Generator is provided, use it; else just use global RNG
        return 0.1 * torch.randn(shape, generator=rng)

    def init_state(self, params: torch.Tensor, rng: Optional[torch.Generator] = None) -> dict:
        """
        Prepares the initial simulation state using the given `params`.
        This function:
         1. Splits params into dynamic (kernel) and initial-cell parts,
         2. Sigmoids them (with optional clipping),
         3. Expresses them into the Lenia simulation (carry),
         4. Optionally runs one step to get a non-blank initial image (like JAX code).
        """
        # Example indices: in JAX code, you used first 45 as dynamics, rest as init cells.
        # You may need to adjust the index (45) to match your actual genotype dimensioning.
        dynamic_len = self.lenia.n_params  # e.g., p*k in your PyTorch code
        base_dyn, base_init = self.base_params[:dynamic_len], self.base_params[dynamic_len:]
        params_dyn, params_init = params[:dynamic_len], params[dynamic_len:]

        # Clip and then sigmoid, just like JAX code
        params_dyn = torch.sigmoid(base_dyn + params_dyn.clamp(-self.clip1, self.clip1))
        params_init = torch.sigmoid(base_init + params_init.clamp(-self.clip2, self.clip2))
        full_params = torch.cat([params_dyn, params_init], dim=0)

        # Express genotype into the Lenia world
        carry = self.lenia.express_genotype(self.init_carry, full_params)

        # Build a 'state' dict and run one step so the initial image isn't blank
        state = dict(carry=carry, img=None)
        state = self.step_state(state, full_params, rng=rng)
        return state

    def step_state(self, state: dict, params: torch.Tensor, rng: Optional[torch.Generator] = None) -> dict:
        """
        Advances the simulation one step. Records a phenotype image
        (cropped to phenotype_size) in `state['img']`.
        """
        carry = state["carry"]
        # The 'unused' argument in JAX is irrelevant in PyTorch, so we pass None
        carry, accum = self.lenia.step(
            carry,
            unused=None,
            phenotype_size=self.phenotype_size,
            center_phenotype=self.center_phenotype,
            record_phenotype=True
        )
        # accum.phenotype is the cropped image
        state["carry"] = carry
        state["img"] = accum.phenotype  # May be None if record_phenotype=False
        return state

    def render_state(self, state: dict, params: torch.Tensor, img_size: Optional[int] = None) -> torch.Tensor:
        """
        Returns the current image from the state. Optionally resizes it
        to `img_size x img_size`.
        """
        img = state["img"]
        if img is None:
            # Possibly means no phenotype was recorded, or step_state never called
            return torch.zeros((self.phenotype_size, self.phenotype_size, 3))

        if img_size is not None and img_size != img.shape[0]:
            # Use nearest-neighbor up/down-sampling
            img = img.permute(2, 0, 1).unsqueeze(0)  # BCHW
            img = F.interpolate(img, size=(img_size, img_size), mode='nearest')
            img = img[0].permute(1, 2, 0)  # back to HWC

        return img

def rollout_simulation(
    rng: Optional[torch.Generator],
    params: torch.Tensor,
    s0: Optional[dict] = None,
    substrate=None,
    fm=None,
    rollout_steps: int = 256,
    time_sampling: Union[str, int, Tuple[int, bool]] = 'final',
    img_size: int = 224,
    return_state: bool = False
):
    """
    Roll out a simulation under a PyTorch-based Lenia substrate, returning
    final or intermediate frames, embeddings, and optional internal state.

    Parameters
    ----------
    rng : torch.Generator or None
        RNG seed for the rollout. If None, global RNG is used.
    params : torch.Tensor
        Genotype parameters for the substrate.
    s0 : dict or None
        If provided, use as initial simulation state. Otherwise, call substrate.init_state(rng, params).
    substrate : object
        The substrate object (PyTorch-based), providing:
          - init_state(rng, params) -> state
          - step_state(rng, state, params) -> new_state
          - render_state(state, params, img_size) -> torch.Tensor image
    fm : object or None
        A "foundation model" object with a method fm.embed_img(image)-> embedding vector.
        If None, no embeddings are computed.
    rollout_steps : int
        Number of timesteps to run the simulation.
    time_sampling : str or int or (int, bool)
        - 'final': returns only final state data.
        - 'video': returns data at each timestep (a list).
        - int (K): returns K equally spaced states across the entire rollout.
        - (K, chunk_ends): if chunk_ends=True, sampling is offset to the end. Matches your JAX logic.
    img_size : int
        Height/width for rendered images (e.g. 224 for CLIP).
    return_state : bool
        If True, include the simulation state in each returned step. If False, only return image/embedding.

    Returns
    -------
    A dictionary or list of dictionaries with keys:
      - 'rgb':  (H, W, 3) or list thereof
      - 'z':    embedding or list thereof (if fm is not None)
      - 'state': dict or None (depending on return_state)
    """

    # ----------------------------------------------------------------
    # 1. Setup initial state
    # ----------------------------------------------------------------
    if s0 is None:
        s0 = substrate.init_state(rng, params)

    # If no foundation model, no embedding
    def embed_img_fn(img):
        return None if fm is None else fm.embed_img(img)

    # ----------------------------------------------------------------
    # 2. If time_sampling == 'final'
    #    -> run rollout_steps, return only final frame + embedding
    # ----------------------------------------------------------------
    if time_sampling == 'final':
        state = s0
        for _ in range(rollout_steps):
            state = substrate.step_state(rng, state, params)
        # Render final frame
        img = substrate.render_state(state, params, img_size=img_size)
        z = embed_img_fn(img)
        return {
            'rgb': img,
            'z': z,
            'state': state if return_state else None
        }

    # ----------------------------------------------------------------
    # 3. If time_sampling == 'video'
    #    -> store data at each timestep
    # ----------------------------------------------------------------
    elif time_sampling == 'video':
        # We'll store a dict of data at each step (0..rollout_steps-1)
        # final state is not strictly included unless we want it.
        data_list = []
        state = s0
        for step_i in range(rollout_steps):
            # Render current state
            img = substrate.render_state(state, params, img_size=img_size)
            z = embed_img_fn(img)
            data_entry = {
                'rgb': img,
                'z': z,
                'state': state if return_state else None
            }
            data_list.append(data_entry)
            # Step to next state
            state = substrate.step_state(rng, state, params)

        return data_list

    # ----------------------------------------------------------------
    # 4. If time_sampling == K or (K, chunk_ends)
    #    -> run rollout_steps, then sample K intervals or chunk_ends
    # ----------------------------------------------------------------
    elif isinstance(time_sampling, int) or (
        isinstance(time_sampling, tuple)
        and len(time_sampling) == 2
        and isinstance(time_sampling[0], int)
        and isinstance(time_sampling[1], bool)
    ):
        if isinstance(time_sampling, int):
            K = time_sampling
            chunk_ends = False
        else:
            K, chunk_ends = time_sampling

        # We'll store the entire trajectory to sample from it
        states = []
        state = s0
        states.append(state)
        for _ in range(rollout_steps):
            state = substrate.step_state(rng, state, params)
            states.append(state)

        # states list length = rollout_steps+1 (initial + each step)

        # Compute sampling indices
        chunk_size = rollout_steps // K
        if chunk_ends:
            # start sampling from chunk_size-1 to rollout_steps
            # i.e. [chunk_size, 2*chunk_size, ..., rollout_steps]
            idx_sample = list(range(chunk_size, rollout_steps+1, chunk_size))
        else:
            # [0, chunk_size, 2*chunk_size, ..., rollout_steps]
            idx_sample = list(range(0, rollout_steps+1, chunk_size))
            # ensure we don't exceed final index
            if idx_sample[-1] > rollout_steps:
                idx_sample[-1] = rollout_steps

        # For each sampled index, render
        data_list = []
        for idx in idx_sample:
            st = states[idx]
            img = substrate.render_state(st, params, img_size=img_size)
            z = embed_img_fn(img)
            data_entry = {
                'rgb': img,
                'z': z,
                'state': st if return_state else None
            }
            data_list.append(data_entry)

        return data_list

    else:
        raise ValueError(f"time_sampling {time_sampling} not recognized")

class FlattenSubstrateParameters:
    """
    Flattens the parameters of a given Lenia substrate into a 1D tensor
    suitable for evolutionary optimization. Mirrors the functionality of
    the JAX-based FlattenSubstrateParameters, but in PyTorch.

    Usage:
      substrate = Lenia(...)               # Your PyTorch-based Lenia
      flat_substrate = FlattenSubstrateParameters(substrate)
      x0 = flat_substrate.default_params() # flattened genotype
      ...
    """

    def __init__(self, substrate):
        """
        :param substrate: An instance of your higher-level Lenia substrate
                          (the PyTorch version).
        """
        self.substrate = substrate
        # We create a default set of params once, to learn shape info.
        with torch.no_grad():
            default_p = self.substrate.default_params()
        # Store shape for flatten/unflatten
        self.original_shape = default_p.shape
        # Count total params
        self.n_params = default_p.numel()

    def flatten_single(self, params: torch.Tensor) -> torch.Tensor:
        """Flatten a single parameter tensor into shape (n_params,)."""
        return params.view(-1)

    def reshape_single(self, flat_params: torch.Tensor) -> torch.Tensor:
        """Reshape a flat parameter vector back to the original param shape."""
        return flat_params.view(self.original_shape)

    def default_params(self, rng: torch.Generator = None) -> torch.Tensor:
        """
        Returns a flat version of the default parameters.
        Equivalent to Flatten -> substrate.default_params().
        """
        params = self.substrate.default_params(rng)
        return self.flatten_single(params)

    def init_state(self, rng: torch.Generator, params: torch.Tensor):
        """
        Unflattens `params` and calls substrate.init_state(...).
        Returns the substrate's initial simulation state.
        """
        params = self.reshape_single(params)
        return self.substrate.init_state(params, rng)

    def step_state(self, rng: torch.Generator, state: dict, params: torch.Tensor):
        """
        Unflattens `params` and calls substrate.step_state(...).
        Returns the new state after one simulation step.
        """
        params = self.reshape_single(params)
        return self.substrate.step_state(state, params, rng)

    def render_state(self, state: dict, params: torch.Tensor, img_size=None):
        """
        Unflattens `params` and calls substrate.render_state(...).
        Returns a rendered image (phenotype).
        """
        params = self.reshape_single(params)
        return self.substrate.render_state(state, params, img_size)

    def __getattr__(self, name):
        """
        Fallback: delegate any undefined attribute lookups
        directly to the substrate object.
        """
        return getattr(self.substrate, name)

rollout_steps = 256
output_path = "a_caterpillar.mp4"
fps = 30

# 2) Create the Lenia substrate
substrate = Lenia(
    grid_size=128,
    center_phenotype=True,
    phenotype_size=64,
    start_pattern="5N7KKM",
    clip1=1.0,
    clip2=1.0
)

# 3) Wrap with flattening so we can handle parameters as a single vector
substrate = FlattenSubstrateParameters(substrate)

# 4) Get default (flat) parameters
with torch.no_grad():
    rng = torch.Generator().manual_seed(0)  # for reproducibility
    default_params = substrate.default_params(rng)

# 5) Run the simulation in 'video' mode to get a list of frames
#    Each item in the returned list is a dict with keys: 'rgb', 'z', 'state'
#    'rgb' is a PyTorch float32 tensor in [0,1]
frames_data = rollout_simulation(
    rng=rng,
    params=default_params,
    s0=None,
    substrate=substrate,  # flattened substrate
    fm=None,              # no foundation model, so no embedding
    rollout_steps=rollout_steps,
    time_sampling='video',  # retrieve each step
    img_size=224,            # smaller for demonstration
    return_state=False
)

# 6) Convert the frames to NumPy arrays in [0..255]
frames_float = [step_dict['rgb'].cpu().numpy() for step_dict in frames_data]
frames_uint8 = [np.clip(frame * 255.0, 0, 255).astype(np.uint8) for frame in frames_float]

# 7) Write to MP4 (H.264) or GIF
#    imageio handles the format by extension. This will write an mp4 video.
imageio.mimsave(output_path, frames_uint8, fps=fps)
print(f"Saved default Lenia simulation video to: {output_path}")

def calc_supervised_target_score(z: torch.Tensor, z_txt: torch.Tensor) -> torch.Tensor:
    T, D = z.shape
    T2 = z_txt.shape[0]
    assert T % T2 == 0, f"Expected T multiple of T2, got T={T}, T2={T2}"

    # Repeat z_txt so that it matches shape T
    k = T // T2
    # shape -> (T, D)
    z_txt_repeated = z_txt.repeat((k, 1))  # or z_txt.unsqueeze(0).repeat(k,1,1).view(-1,D)

    # Compute kernel = z_txt_repeated @ z^T => shape (T, T)
    kernel = torch.matmul(z_txt_repeated, z.t())

    # Negative mean diagonal
    return -torch.diagonal(kernel, 0).mean()


def calc_reconstruction_loss(z_txt: torch.Tensor, z_desc: torch.Tensor) -> torch.Tensor:
    T, D = z_desc.shape
    T2 = z_txt.shape[0]
    assert T % T2 == 0, f"Expected T multiple of T2, got T={T}, T2={T2}"

    # Repeat z_txt so that it matches shape T
    k = T // T2
    z_txt_repeated = z_txt.repeat((k, 1))  # shape -> (T, D)

    kernel = torch.matmul(z_txt_repeated, z_desc.t())  # (T, T)
    return -torch.diagonal(kernel, 0).mean()


def calc_supervised_target_softmax_score(
    z: torch.Tensor,
    z_txt: torch.Tensor,
    temperature_softmax: float = 0.01
) -> torch.Tensor:
    T, D = z.shape
    T2 = z_txt.shape[0]
    assert T % T2 == 0, f"Expected T multiple of T2, got T={T}, T2={T2}"

    # Repeat z_txt to match T
    k = T // T2
    z_txt_repeated = z_txt.repeat((k, 1))  # (T, D)

    # Dot-product kernel => shape (T, T)
    kernel = torch.matmul(z_txt_repeated, z.t())

    # Compute softmax across different dimensions
    # kernel / temperature_softmax => scaled logit
    loss_sm1 = F.softmax(kernel / temperature_softmax, dim=-1)  # shape (T, T)
    loss_sm2 = F.softmax(kernel / temperature_softmax, dim=-2)  # shape (T, T)

    # Negative log of diagonal entries
    # diag(loss_smX) has shape (T,)
    diag_sm1 = torch.diagonal(loss_sm1, 0)
    diag_sm2 = torch.diagonal(loss_sm2, 0)
    # Avoid log(0) => clamp
    loss_sm1 = -torch.log(diag_sm1.clamp_min(1e-12)).mean()
    loss_sm2 = -torch.log(diag_sm2.clamp_min(1e-12)).mean()

    return 0.5 * (loss_sm1 + loss_sm2)


def calc_open_endedness_score(z: torch.Tensor) -> torch.Tensor:
    # kernel shape: (T, T)
    kernel = torch.matmul(z, z.t())

    # Zero out the upper triangle including diagonal
    # so we only keep the strictly lower-triangular part
    # or use torch.tril(kernel, diagonal=-1)
    kernel_lower = torch.tril(kernel, diagonal=-1)

    # row-wise max => shape (T,)
    row_max, _ = kernel_lower.max(dim=-1)

    return row_max.mean()

def asal(
    fm,
    prompts: str = "a caterpillar",
    substrate=None,
    rollout_steps: int = 256,
    n_iters: int = 1000,
    save_dir: str = "./demo_run/",
    seed: int = 42,
    pop_size: int = 16,
    sigma: float = 0.1,
    coef_prompt: float = 0.0,
    coef_softmax: float = 0.0,
    coef_oe: float = 0.0,
    bs: int = 1,
):
    """
    EvoTorch-based PyTorch reimplementation of the JAX function 'asal'.

    Performs a black-box optimization with CMA-ES over the flattened parameters
    of a Lenia substrate, guided by textual prompts via a foundation model `fm`.

    Parameters
    ----------
    fm : FoundationModel
        Foundation model with methods:
          - fm.embed_txt(list_of_strings) -> torch.Tensor shape (N_prompts, embed_dim)
          - fm.embed_img(image_tensor)    -> torch.Tensor shape (embed_dim,)
    prompts : str
        Semicolon-separated textual prompts (e.g. "a butterfly; an orange cat").
    substrate : FlattenSubstrateParameters
        PyTorch-based Lenia substrate, flattened for evolutionary search. If None, a default is created.
    rollout_steps : int
        Number of timesteps to simulate for each candidate solution.
    n_iters : int
        Number of iterations of CMA-ES to run.
    save_dir : str
        Folder path to store partial logs/checkpoints.
    seed : int
        Random seed for CMA-ES.
    pop_size : int
        Population size of CMA-ES.
    sigma : float
        Initial standard deviation (step size) for CMA-ES.
    coef_prompt : float
        Weight of the prompt-based term in the loss.
    coef_softmax : float
        Weight of the softmax-based term in the loss.
    coef_oe : float
        Weight of open-endedness term in the loss.
    bs : int
        Number of random seeds for each solution's rollout (batching). Example demonstrates single-seed.

    Returns
    -------
    dict
        A dictionary containing 'best_fitness', 'best_solution', and 'loss_log'.
    """
    if substrate is None:
        substrate = Lenia(
            grid_size=128,
            center_phenotype=True,
            phenotype_size=64,
            start_pattern="5N7KKM",
            clip1=1.0
        )
        substrate = FlattenSubstrateParameters(substrate)
    else:
        # Ensure substrate is an instance of FlattenSubstrateParameters
        if not hasattr(substrate, "n_params"):
            substrate = FlattenSubstrateParameters(substrate)

    # Convert textual prompts into embeddings
    prompt_list = prompts.split(";")
    z_txt = fm.embed_txt(prompt_list)  # shape: (N_prompts, embed_dim)

    # We define a partial rollout function to gather images+embeddings across time
    rollout_fn = partial(
        rollout_simulation,
        rng=None,          # We'll rely on inside calls or single-seed
        s0=None,
        substrate=substrate,
        fm=fm,
        rollout_steps=rollout_steps,
        time_sampling="video",  # gather all frames
        img_size=224,
        return_state=False
    )

    # -------------------------------------------------------------------------
    # 2.2) Create an EvoTorch Problem
    # -------------------------------------------------------------------------
    class LeniaProblem(Problem):
        def __init__(self):
            # We treat it as a minimization problem
            super().__init__(
                objective_sense="min",
                solution_length=substrate.n_params,
                # Optionally define initial_bounds. In JAX code, you had no explicit bounds,
                # so we can do None or some range. We'll do a small range for demonstration:
                initial_bounds=(-1.0, 1.0),
                # We can define device / dtype if needed:
                # device="cpu", dtype=torch.float32
            )

        def _evaluate_batch(self, solutions: SolutionBatch) -> None:
            """
            Evaluate a batch of solutions. This method is called by EvoTorch each iteration.
            solutions.values has shape (batch_size, n_params).
            We must compute a float loss for each solution and call solutions.set_evals(...).
            """
            x = solutions.values  # shape: (batch_size, n_params)
            batch_size = x.shape[0]

            # We'll store the final loss in a 1D tensor of shape [batch_size].
            losses = torch.zeros(batch_size, device=x.device, dtype=x.dtype)

            for i in range(batch_size):
                params_i = x[i]
                # If you want multiple seeds (bs) for each solution, you'd do that in a small loop:
                # but here we do a single rollout for brevity
                rollout_data = rollout_fn(params=params_i)

                # rollout_data is a list of dicts (time_sampling='video').
                # Each dict has: 'rgb' -> image, 'z' -> embedding
                # Collect the 'z' from each frame:
                z_frames = [f['z'] for f in rollout_data if f['z'] is not None]
                if len(z_frames) == 0:
                    # If fm.embed_img returned None, define z = None => no embeddings
                    z = None
                else:
                    # shape: (T, embed_dim)
                    z = torch.stack(z_frames, dim=0)

                # Compute the combined objective:
                # Weighted sum of different sub-losses
                loss_i = torch.tensor(0.0, device=x.device, dtype=x.dtype)

                if coef_prompt > 0.0:
                    loss_prompt = calc_supervised_target_score(z, z_txt)
                    loss_i += coef_prompt * loss_prompt

                if coef_softmax > 0.0:
                    loss_softmax = calc_supervised_target_softmax_score(z, z_txt)
                    loss_i += coef_softmax * loss_softmax

                if coef_oe > 0.0:
                    loss_oe = calc_open_endedness_score(z)
                    loss_i += coef_oe * loss_oe

                losses[i] = loss_i

            solutions.set_evals(losses)

    problem = LeniaProblem()

    # -------------------------------------------------------------------------
    # 2.3) Create CMA-ES searcher
    # -------------------------------------------------------------------------
    # We'll use CMAES from evotorch.algorithms.
    # If you want to use diagonal CMA, pass `separable=True`.
    searcher = CMAES(
        problem,
        popsize=pop_size,
        stdev_init=sigma,
        #random_seed=seed,
    )

    # Attach a logger that prints to stdout
    logger = StdOutLogger(searcher)

    # Optionally create save_dir
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)

    # -------------------------------------------------------------------------
    # 2.4) Run the optimization
    # -------------------------------------------------------------------------
    # We'll store the best solution each iteration in a Python list for demonstration
    best_losses = []

    for iteration in range(n_iters):
        # Evolve one iteration
        searcher.step()
        # Current best fitness in the population
        current_best = searcher.status["best_eval"]

        best_losses.append(current_best)

        # Periodically save
        # e.g. every 10% or final iteration
        if iteration % max(1, (n_iters // 10)) == 0 or iteration == (n_iters - 1):
            print(f"[Iteration {iteration}] best_loss = {current_best:.4f}")

            # Save checkpoint
            # import pdb; pdb.set_trace()
            best_solution_vector = searcher.population[0].values.clone().detach().cpu()
            if save_dir:
                ckpt_path = os.path.join(save_dir, f"ckpt_iter_{iteration}.pt")
                torch.save({
                    "iter": iteration,
                    "best_solution_vector": best_solution_vector
                }, ckpt_path)
                torch.save({
                    "iter": iteration,
                    "best_fitness": current_best,
                    "best_solution": best_solution_vector,
                    "loss_log": np.array(best_losses),
                }, ckpt_path)

    # -------------------------------------------------------------------------
    # 2.5) Summarize final results
    # -------------------------------------------------------------------------
    best_fitness = searcher.status["best_eval"]
    best_solution = searcher.status["best_solution"].clone().detach().cpu()

    print("==== Optimization Finished ====")
    print(f"Best Fitness: {best_fitness.item():.5f}")
    print(f"Best Solution Param shape: {best_solution.shape}")

    return {
        "best_fitness": best_fitness.item(),
        "best_solution": best_solution,
        "loss_log": best_losses
    }

results = asal(
    fm=fm,
    prompts="a caterpillar",     # textual prompt(s)
    substrate=None,             # let asal create a default substrate
    rollout_steps=64,           # fewer steps for a quick demo
    n_iters=10,                 # small number of iterations
    save_dir=None,              # do not save to disk
    seed=42,
    pop_size=8,
    sigma=0.2,
    coef_prompt=1.0,            # weighting for prompt-based objective
    coef_softmax=0.0,           # weighting for softmax objective
    coef_oe=0.0,                # weighting for open-endedness objective
    bs=1,
)

print("\n==== ASAL Demo Finished ====")
print("Best Fitness:", results["best_fitness"])
print("Best Solution (first 10 dims):", results["best_solution"][:10])
print("Loss Log:", results["loss_log"])

